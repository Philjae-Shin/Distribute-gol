# Distributed Game of Life Implementation

## üì° Overview

The distributed implementation extends the parallel Game of Life to work across multiple machines over a network, enabling massive grid sizes and computational scalability beyond single-machine limitations.

## Report
![Page 1](./content/report_1.png)
![Page 2](./content/report_2.png)
![Page 3](./content/report_3.png)
![Page 4](./content/report_4.png)
![Page 5](./content/report_5.png)
![Page 6](./content/report_6.png)

## üèóÔ∏è Architecture

### System Architecture
![Distributed Architecture](./content/cw_diagrams-Distributed_1.png)

The distributed system follows a master-worker architecture:
- **Broker Node**: Central coordinator managing work distribution and result aggregation
- **Worker Nodes**: Compute nodes processing assigned grid sections
- **RPC Communication**: Remote Procedure Call protocol for network communication
- **AWS Integration**: Optional cloud deployment for elastic scaling

### Network Topology
![Network Topology](content/cw_diagrams-Distributed_2.png)

The system implements a star topology with the broker at the center:
```
         Broker (Master)
        /      |      \
       /       |       \
   Worker1  Worker2  Worker3 ... WorkerN
```

Each worker maintains a persistent connection to the broker for:
- Receiving work assignments
- Sending computation results
- Heartbeat monitoring
- State synchronization

## üöÄ Key Features

### Distributed Computing Capabilities
- **Horizontal Scaling**: Add workers dynamically for larger grids
- **Load Balancing**: Automatic work distribution based on worker capacity
- **Fault Tolerance**: Handles worker failures gracefully
- **State Persistence**: Checkpoint and recovery mechanisms
- **Network Optimization**: Compressed data transfer for efficiency

### Advanced Features
- **Dynamic Worker Registration**: Workers can join/leave during execution
- **Adaptive Partitioning**: Grid division based on worker performance
- **Asynchronous Communication**: Non-blocking network operations
- **Result Caching**: Minimize redundant computations

## üíª Implementation Details (Code Examples)

### RPC Protocol Design

#### Worker Registration (Code Example)
```go
type RegisterRequest struct {
    WorkerID    string
    Capacity    int
    IPAddress   string
    Port        int
}

type RegisterResponse struct {
    Accepted    bool
    WorkerIndex int
    Config      WorkerConfig
}
```

#### Work Distribution (Code Example)
```go
type WorkRequest struct {
    Generation  int
    StartY      int
    EndY        int
    WorldData   [][]byte
}

type WorkResponse struct {
    WorkerID    string
    Generation  int
    StartY      int
    EndY        int
    ResultData  [][]byte
    AliveCount  int
}
```

### Communication Flow

#### Initial Setup Phase
![Setup Phase](content/cw_diagrams-Distributed_3.png)

1. **Broker Initialization**
   - Start RPC server on designated port
   - Initialize world state from input file
   - Prepare work distribution strategy

2. **Worker Registration**
   - Workers connect to broker via RPC
   - Exchange capability information
   - Receive initial configuration

3. **Work Assignment**
   - Broker partitions grid based on worker count
   - Assigns sections to each worker
   - Sends initial world state

#### Computation Phase
![Computation Phase](content/cw_diagrams-Distributed_4.png)

For each generation:
1. **Distribution**
   - Broker sends current world sections to workers
   - Includes halo regions for boundary calculations
   
2. **Parallel Processing**
   - Workers compute next generation for their sections
   - Apply Game of Life rules independently
   
3. **Result Collection**
   - Workers send results back to broker
   - Broker assembles complete world state
   
4. **Synchronization**
   - Broker ensures all workers complete before next generation
   - Handles any failed workers by reassigning work

#### Fault Tolerance
![Fault Tolerance](content/cw_diagrams-Distributed_5.png)

The system implements multiple fault tolerance mechanisms:

1. **Heartbeat Monitoring**
   ```go
   func (b *Broker) monitorWorker(workerID string) {
       ticker := time.NewTicker(5 * time.Second)
       for range ticker.C {
           if !b.pingWorker(workerID) {
               b.handleWorkerFailure(workerID)
           }
       }
   }
   ```

2. **Work Reassignment**
   - Detect failed workers via timeout or heartbeat
   - Redistribute work among healthy workers
   - Continue computation without interruption

3. **State Checkpointing**
   - Periodic snapshots of world state
   - Enable recovery from broker failures
   - Minimize lost computation on restart

## üîß Configuration

### Broker Configuration
```yaml
broker:
  port: 8030
  max_workers: 16
  timeout: 30s
  checkpoint_interval: 100
  compression: true
```

### Worker Configuration
```yaml
worker:
  broker_address: "localhost:8030"
  worker_id: "worker-1"
  buffer_size: 1024
  retry_attempts: 3
  heartbeat_interval: 5s
```

## üö¶ Running the Distributed System

### Starting the Broker
```bash
# Run broker on main machine
go run . -mode=broker -port=8030 -w=1024 -h=1024 -turns=10000
```

### Starting Workers
```bash
# Run on worker machines (or multiple terminals)
go run . -mode=worker -broker=192.168.1.100:8030 -id=worker1
go run . -mode=worker -broker=192.168.1.100:8030 -id=worker2
go run . -mode=worker -broker=192.168.1.100:8030 -id=worker3
```

### AWS Deployment
```bash
# Deploy to AWS using provided scripts
./scripts/deploy-aws.sh --instances=4 --type=t2.micro
```

## üìä Performance Metrics

### Scalability Analysis
| Workers | Grid Size | Turns/Second | Speedup | Efficiency |
|---------|-----------|--------------|---------|------------|
| 1       | 512x512   | 15           | 1.0x    | 100%       |
| 2       | 512x512   | 28           | 1.87x   | 93.5%      |
| 4       | 512x512   | 52           | 3.47x   | 86.7%      |
| 8       | 512x512   | 95           | 6.33x   | 79.1%      |
| 16      | 512x512   | 168          | 11.2x   | 70.0%      |

### Network Overhead
- **Latency Impact**: ~2-5ms per generation for LAN
- **Bandwidth Usage**: Compressed transfer reduces by 60-80%
- **Optimal Worker Count**: 1 worker per 128x128 section

## üîç Monitoring and Debugging

### Distributed Tracing
```go
// Enable tracing for performance analysis
func (b *Broker) traceGeneration(gen int) {
    span := trace.StartSpan("generation", trace.WithAttributes(
        attribute.Int("generation", gen),
        attribute.Int("workers", len(b.workers)),
    ))
    defer span.End()
    // ... computation logic
}
```

### Metrics Collection
- **Prometheus Integration**: Export metrics for monitoring
- **Grafana Dashboards**: Visualize system performance
- **Log Aggregation**: Centralized logging with correlation IDs

### Debug Commands
```bash
# View broker status
curl http://broker:8030/status

# List active workers
curl http://broker:8030/workers

# Force checkpoint
curl -X POST http://broker:8030/checkpoint
```

## üõ†Ô∏è Advanced Topics

### Custom Partitioning Strategies
```go
type PartitionStrategy interface {
    Partition(width, height, workers int) []WorkSection
}

// Implement different strategies
type StripPartition struct{}     // Horizontal strips
type BlockPartition struct{}     // 2D blocks
type AdaptivePartition struct{}  // Based on density
```

### Compression Algorithms
- **RLE (Run-Length Encoding)**: For sparse grids
- **Bit-packing**: For dense grids
- **Delta Encoding**: For small changes between generations

### Security Considerations
- **TLS Encryption**: Secure communication channels
- **Authentication**: Worker verification tokens
- **Rate Limiting**: Prevent DoS attacks

## üéØ Optimization Strategies

### Communication Optimization
1. **Batch Updates**: Send multiple generations in one RPC
2. **Async Processing**: Pipeline computation and communication
3. **Smart Caching**: Cache unchanged sections

### Computation Optimization
1. **Sparse Representation**: Skip empty regions
2. **SIMD Instructions**: Vectorized cell processing
3. **GPU Offloading**: CUDA kernels for large sections

## üìà Benchmarking

### Running Benchmarks
```bash
# Benchmark different worker counts
./scripts/benchmark-distributed.sh --workers=1,2,4,8,16

# Network overhead analysis
go test -bench=BenchmarkNetworkOverhead

# Fault tolerance testing
./scripts/chaos-test.sh --kill-workers=2
```

### Results Visualization
Generate performance graphs:
```python
python scripts/plot_performance.py --input=bench_results.json
```

## üîó Integration Points

### REST API Endpoints
```
GET  /status          - System status
GET  /workers         - List workers
POST /compute         - Start computation
GET  /result/{id}     - Get computation result
POST /checkpoint      - Force checkpoint
```

### Webhook Support
```go
type WebhookConfig struct {
    URL       string
    Events    []string  // ["generation_complete", "worker_failed"]
    AuthToken string
}
```

## üö® Error Handling

### Common Issues and Solutions

| Issue | Cause | Solution |
|-------|-------|----------|
| Worker timeout | Network latency | Increase timeout values |
| Unbalanced load | Uneven partitioning | Use adaptive partitioning |
| Memory overflow | Large grid size | Enable compression |
| Connection refused | Firewall/Port issues | Check network configuration |

## üìö Further Reading

- [Distributed Systems Principles](https://www.distributed-systems.net/)
- [Go RPC Documentation](https://golang.org/pkg/net/rpc/)
- [AWS EC2 Best Practices](https://docs.aws.amazon.com/ec2/)
- [Network Programming with Go](https://tumregels.github.io/Network-Programming-with-Go/)

## ü§ù Contributing

Areas for improvement in the distributed implementation:
- **P2P Architecture**: Remove single point of failure
- **Kubernetes Operator**: Cloud-native deployment
- **gRPC Migration**: Modern RPC framework
- **Consensus Algorithms**: Multi-master support
- **Edge Computing**: Support for IoT devices
