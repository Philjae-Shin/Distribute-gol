# Distributed Game of Life Implementation

## üåê Overview

A high-performance distributed implementation of Conway's Game of Life that scales across multiple machines using Go's RPC framework. This system enables processing of massive grids by distributing computation across a network of worker nodes.

## ‚ú® Key Features

- **Multi-node Architecture**: Distributed processing across AWS EC2 instances or local network
- **Client-Server Separation**: Decoupled controller and computation engine
- **Real-time Monitoring**: Live statistics and board state tracking
- **Fault Resilience**: Graceful handling of node failures
- **Scalable Design**: Linear performance scaling with additional nodes
- **Network Optimized**: Efficient halo exchange and data compression

## Report
![Page 1](./content/report_1.png)
![Page 2](./content/report_2.png)
![Page 3](./content/report_3.png)
![Page 4](./content/report_4.png)
![Page 5](./content/report_5.png)
![Page 6](./content/report_6.png)

## üèóÔ∏è System Architecture

### Core Components

![Distributed Architecture Overview](content/cw_diagrams-Distributed_1.png)

The system consists of three main components:

1. **Local Controller (Client)**
   - Handles all I/O operations and user interactions
   - Manages PGM image input/output
   - Captures and processes keyboard commands
   - Displays SDL window for visualization (optional)

2. **GOL Engine (Server)**
   - Processes Game of Life evolution logic
   - Manages the complete world state
   - Coordinates with worker nodes
   - Handles RPC communication

3. **Communication Layer**
   - RPC-based protocol for client-server communication
   - Efficient binary serialization
   - Asynchronous event streaming

### Distributed Processing Pipeline

#### Live Monitoring System
![Real-time Monitoring](content/cw_diagrams-Distributed_2.png)

The system provides continuous monitoring capabilities:
- **Periodic Updates**: Alive cell count reported every 2 seconds
- **Non-blocking Events**: Asynchronous event channel communication
- **Performance Metrics**: Real-time throughput and latency tracking
- **Health Checks**: Continuous node status monitoring

#### State Management
![State Management](content/cw_diagrams-Distributed_3.png)

Efficient state handling across the distributed system:
- **Persistent State**: GOL engine maintains world state between client sessions
- **Snapshot Support**: On-demand PGM output of current state
- **State Recovery**: Checkpoint and restore mechanisms
- **Memory Optimization**: Efficient buffer management for large grids

#### Interactive Control System
![Control System](content/cw_diagrams-Distributed_4.png)

Advanced control features for system management:

**Keyboard Commands:**
- `S` - Save current board state as PGM image
- `Q` - Gracefully disconnect client (server continues)
- `K` - Complete system shutdown with final state save
- `P` - Pause/Resume processing toggle

The control system ensures:
- Clean client disconnection without server errors
- Multiple client support (sequential connections)
- State preservation across client sessions
- Responsive command handling during computation

### Multi-Worker Architecture

![Worker Distribution](content/cw_diagrams-Distributed_5.png)

The distributed computation layer:
- **Dynamic Work Distribution**: Automatic partitioning based on worker count
- **Load Balancing**: Even distribution of computational load
- **Parallel Processing**: Independent worker nodes process grid sections
- **Result Aggregation**: Efficient collection and merging of results

### Broker-Based Architecture

![Broker Pattern](content/cw_diagrams-Distributed_6.png)

Advanced broker pattern implementation:
- **Centralized Coordination**: Broker manages all worker communications
- **Decoupled Design**: Controller and workers communicate only through broker
- **Scalability**: Easy addition/removal of worker nodes
- **Fault Isolation**: Worker failures don't affect controller

## üöÄ Getting Started

### Prerequisites
```bash
# Install Go 1.19+
go version

# Set up AWS CLI (for cloud deployment)
aws configure
```

## üíª Usage

### Local Development

#### Start the GOL Server
```bash
# Terminal 1: Start the GOL engine server
go run ./server -port 8030
```

#### Run the Controller Client
```bash
# Terminal 2: Start the controller
go run ./client -server localhost:8030 -w 512 -h 512 -turns 1000
```

### Distributed Deployment

#### Launch Broker Node
```bash
# On broker machine (or AWS instance)
./gol-distributed broker --port 8030 --workers 4
```

#### Start Worker Nodes
```bash
# On each worker machine
./gol-distributed worker --broker broker-ip:8030 --id worker-1
./gol-distributed worker --broker broker-ip:8030 --id worker-2
# ... repeat for additional workers
```

#### Connect Controller
```bash
# On local machine
./gol-distributed controller --broker broker-ip:8030 --image game.pgm
```

### Configuration Options
```yaml
# config.yaml
controller:
  server: "localhost:8030"
  timeout: 30s
  
engine:
  port: 8030
  max_connections: 10
  buffer_size: 4096
  
workers:
  count: 4
  threads_per_worker: 2
  halo_size: 1
```

## üîß Implementation (High level code)

### RPC Protocol (Code Example)

The system uses Go's built-in RPC framework with custom message types:

```go
// Core RPC methods
type GOLService interface {
    // Start processing with given parameters
    StartEvolution(params Params, reply *Board) error
    
    // Get current alive cells count
    GetAliveCells(turn int, count *int) error
    
    // Pause/Resume processing
    TogglePause(pause bool, reply *bool) error
    
    // Save current state
    SaveBoard(filename string, reply *bool) error
    
    // Shutdown system
    Shutdown(save bool, reply *Board) error
}
```

### Network Communication

#### Data Transfer Optimization
- **Halo Exchange**: Only boundary rows/columns transmitted between workers
- **Delta Encoding**: Send only changed cells for sparse updates
- **Compression**: Optional gzip compression for large boards
- **Batching**: Multiple small messages combined into single transmission

#### Message Flow
```
Controller -> Broker: StartGame(params)
Broker -> Workers: DistributeWork(sections)
Workers -> Broker: ReturnResults(computed)
Broker -> Controller: UpdateState(newBoard)
```

### Performance Optimizations

#### Computation
- **Strip Partitioning**: Optimal cache-friendly data layout
- **Double Buffering**: Eliminate allocation overhead
- **SIMD Operations**: Vectorized cell counting (where available)

#### Communication
- **Async I/O**: Non-blocking network operations
- **Connection Pooling**: Reuse TCP connections
- **Protocol Buffers**: Efficient binary serialization (optional)

## üìä Performance Benchmarks

### Scalability Results

Testing with 512x512 grid, 1000 iterations:

| Workers | Time (s) | Speedup | Efficiency |
|---------|----------|---------|------------|
| 1       | 120.5    | 1.0x    | 100%       |
| 2       | 62.3     | 1.93x   | 96.5%      |
| 4       | 32.1     | 3.75x   | 93.8%      |
| 8       | 17.2     | 7.01x   | 87.6%      |
| 16      | 9.8      | 12.3x   | 76.9%      |

### Network Overhead Analysis

For 1024x1024 grid with 8 workers:
- **Halo Exchange**: 2.3% of total time
- **Result Collection**: 1.1% of total time  
- **Serialization**: 0.8% of total time
- **Network Latency**: 3.2ms average (LAN)

### Large-Scale Testing

Successfully tested with:
- **Maximum Grid**: 5120x5120 pixels
- **Maximum Workers**: 64 nodes
- **Maximum Throughput**: 2.1 billion cells/second

## üî® Advanced Features

### Halo Exchange Optimization

![Halo Exchange](content/cw_diagrams-Extensions_1.png)

Direct peer-to-peer halo exchange between adjacent workers:
- Eliminates broker bottleneck for boundary data
- Reduces network traffic by 40%
- Enables overlap of computation and communication
- Supports both synchronous and asynchronous modes

### Hybrid Parallel-Distributed System

![Hybrid Architecture](content/cw_diagrams-Extensions_2.png)

Combines distributed and parallel approaches:
- Multiple threads per distributed node
- Optimal resource utilization on multi-core systems
- Configurable thread-to-worker ratio
- Automatic load balancing between threads and nodes

### Live SDL Visualization

Real-time visualization of distributed computation:
- Streaming cell updates from remote nodes
- Adaptive frame rate based on network latency
- Efficient delta-based rendering
- Optional compression for slow networks

### Fault Tolerance Mechanisms

Comprehensive failure handling:
- **Heartbeat Monitoring**: Detect failed nodes within 5 seconds
- **Work Redistribution**: Automatic reassignment of failed worker's tasks
- **State Checkpointing**: Periodic snapshots for recovery
- **Graceful Degradation**: Continue with reduced workers

## üêõ Debugging and Monitoring

### Logging
```bash
# Enable debug logging
export GOL_LOG_LEVEL=debug

# Log to file
./gol-distributed 2> gol.log
```

### Performance Profiling
```bash
# CPU profiling
go run . -cpuprofile cpu.prof

# Memory profiling
go run . -memprofile mem.prof

# Analyze profiles
go tool pprof cpu.prof
```

### Network Monitoring
```bash
# Monitor RPC calls
tcpdump -i any port 8030

# Check connection status
netstat -an | grep 8030
```

## ü§ù Contributing

Areas for improvement:

### Priority Features
- **gRPC Migration**: Replace RPC with gRPC for better performance
- **Kubernetes Support**: Container orchestration for cloud deployment  
- **WebAssembly Client**: Browser-based controller
- **GPU Acceleration**: CUDA/OpenCL worker nodes
- **Adaptive Partitioning**: Dynamic load balancing based on cell density

### Development Workflow
1. Fork the repository
2. Create a feature branch
3. Implement your feature with tests
4. Submit a pull request with benchmarks

### Testing
```bash
# Run unit tests
go test ./...

# Run integration tests  
go test -tags=integration

# Run benchmarks
go test -bench=. -benchmem
```

## üìö References

- [Building Distributed Systems in Go](https://www.oreilly.com/library/view/distributed-systems-with/9781492077834/)
- [Go RPC Package Documentation](https://pkg.go.dev/net/rpc)
- [Conway's Game of Life Algorithms](https://www.conwaylife.com/wiki/Cellular_automaton)
- [Halo Exchange in Parallel Computing](https://en.wikipedia.org/wiki/Halo_exchange)

## üôè Acknowledgments

- Conway's Game of Life community for patterns and test cases
- Go team for the excellent RPC framework
- Contributors and testers from the open source community
